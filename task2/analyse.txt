WICHTIG:
DIE GLOVE DATEIEN SIND ZU GROSS FÜR GITHUB. DESHALB HABE ICH DIESE DATEIEN NICHT HIER.

b) Es ist nicht leicht zu quantifizieren, aber ich habe das Gefühl, dass das System mit GloVe eher das Rechenergebnis liefert, das ich erwarte. Oft ist bei dem Embeddingsystem das Problem, dass das Ergebnis zu nah an einem der beteiligten Wörtern ist. z.B. 'Instagram - pictures + hate' ergibt 'hatred', 'hates', ... Ergebnisse die auch irgendwie Sinn machen aber nicht das sind, was erwartet wäre. GloVe gibt Twitter als eines der ersten Ergebnisse zurück. Allerdings kann das SentenceTransformer Modell auch mit mehrteiligen Begriffen umgehen, die in GloVe nicht gegeben sind. Mit einem Entsprechenden Wörterbuch funktioniert auch das Embeddingsystem wie erwartet. D.h. wenn man das Model aus einigen Ausgewählten Wörtern auswählen lässt. In dieser Implementierung hab ich mich aber in einer späteren Interation dafür entschieden das Vokabular vom Model selbst erstellen zu lassen um mehr Flexibilität mit den Rechnungen zu haben.

c) Für sinnvoll halte ich Plus und Minus Operation. Auch wenn man Vektoren Multiplizieren kann. Das Kreuzprodukt bildet aber einen Vektor, der Orthogonal zu den beiden Anderen steht und sich somit potentiell eher vom Sinnzusammenhang entfernt. Rein von der Wortbedeutung her macht es Sinn 'Weiblichkeit' oder 'Adel' zu addieren um aus 'Mensch' 'Königin' zu machen aber Multiplizieren oder Dividieren macht keinen Sinn. Interessant wäre eventuell noch mit Skalaren zu multiplizieren. Also das vielfache von 'gut' oder 'schlecht' zu nehmen. 

